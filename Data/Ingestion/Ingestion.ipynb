{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f06b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Setup\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Google Gemini (Generative AI) SDK\n",
    "import google.generativeai as genai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load API keys from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set Gemini API key\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"../Transcriptions/EMMA_ES-TRANSCRIPTIONS\")\n",
    "KB_DIR = Path(\"../../KB/chroma_db\")\n",
    "KB_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e60cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text & remove basic PII (emails, phones, card numbers).\n",
    "    For production, replace with Presidio or SpaCy NER for stronger PII detection.\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove common PII patterns\n",
    "    text = re.sub(r\"\\b[\\w._%+-]+@[\\w.-]+\\.\\w{2,}\\b\", \"[EMAIL]\", text)  # emails\n",
    "    text = re.sub(r\"\\b\\d{10,16}\\b\", \"[NUMBER]\", text)  # phone/credit card numbers\n",
    "    text = re.sub(r\"\\b\\d{3}[-.\\s]?\\d{2}[-.\\s]?\\d{4}\\b\", \"[ID]\", text)  # SSN-like patterns\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_transcripts(path: Path):\n",
    "    \"\"\"\n",
    "    Load all transcript .txt files from folder\n",
    "    \"\"\"\n",
    "    files = glob.glob(str(path / \"*.txt\"))\n",
    "    transcripts = []\n",
    "    for f in files:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as file:\n",
    "            transcripts.append({\"file\": os.path.basename(f), \"text\": file.read()})\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356654b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 transcripts\n",
      "¿qué tal? aquí le dije más colchones. ¿cómo le puedo ayudar? ah, buenas. oiga, este, mire, quería comprar un colchón individual, pero el essential, o no sé cómo se le llama, ¿no? sí, el lema essential. ajá, entonces lo iba a hacer por la red, pero vi que el sistema de pago es paypal. entonces quería saber si hay alguna otra forma de pago que no sea paypal, que me cae gordo. ay, a ver, a ver, a ver. quiero comprar un colchón essential, pero por la red, o sea, cuando me metí a la página de ustedes\n"
     ]
    }
   ],
   "source": [
    "# Load & Clean Transcripts\n",
    "\n",
    "raw_data = load_transcripts(BASE_DIR)\n",
    "\n",
    "print(f\"Loaded {len(raw_data)} transcripts\")\n",
    "\n",
    "# Clean\n",
    "for doc in raw_data:\n",
    "    doc[\"clean_text\"] = clean_text(doc[\"text\"])\n",
    "\n",
    "print(raw_data[0][\"clean_text\"][:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "532862d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 chunks from transcripts\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for doc in raw_data:\n",
    "    chunks = text_splitter.split_text(doc[\"clean_text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        docs.append({\n",
    "            \"id\": f\"{doc['file']}_chunk{i}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\"source\": doc[\"file\"], \"chunk_id\": i}\n",
    "        })\n",
    "\n",
    "print(f\"Generated {len(docs)} chunks from transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ad2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2148\\3889173006.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "d:\\AI-Calling-Agent\\aenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2148\\3889173006.py:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knowledge Base updated & stored in KB/chroma_db/ with SentenceTransformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2148\\3889173006.py:23: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Embeddings + Vector DB (using SentenceTransformers)\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Choose a multilingual embedding model (works with Spanish + English)\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    ")\n",
    "\n",
    "# Initialize Chroma vector DB\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"transcripts_kb\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=str(KB_DIR)\n",
    ")\n",
    "\n",
    "# Add docs into Chroma\n",
    "texts = [d[\"text\"] for d in docs]\n",
    "metadatas = [d[\"metadata\"] for d in docs]\n",
    "ids = [d[\"id\"] for d in docs]\n",
    "\n",
    "vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"✅ Knowledge Base updated & stored in KB/chroma_db/ with SentenceTransformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23cb6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Source: 2020_12_21_LUFR_529988843072___EMMA_ES__.txt\n",
      "en mercado libre, ya lo entiendo ahora. o sea, no estoy hablando directamente a la fábrica o a la cliente. perdón, me hice entender mal. o sea, nosotros también nos encontramos en mercado libre. ajá, ok. entonces, ajá, quería ver si solo se puede pagar con paypal o si se puede pagar con tarjeta de d\n",
      "----\n",
      "Source: 2020_12_21_LUFR_529988843072___EMMA_ES__.txt\n",
      "¿qué tal? aquí le dije más colchones. ¿cómo le puedo ayudar? ah, buenas. oiga, este, mire, quería comprar un colchón individual, pero el essential, o no sé cómo se le llama, ¿no? sí, el lema essential. ajá, entonces lo iba a hacer por la red, pero vi que el sistema de pago es paypal. entonces quería\n",
      "----\n",
      "Source: 2020_12_21_LUFR_529988843072___EMMA_ES__.txt\n",
      "tarjeta de crédito acepta de visa, american express, mastercard, tarjetas de débito, también con mastercard débito, visa de débito, mercado pago. tiene más métodos de pago que serían efectivo. oxo, pagas y sacrajetas internacionalmente. y por último, depósitos de transferencia bancaria. santander, v\n",
      "----\n",
      "Source: 2021_04_05_DANO__34919016816___EMMA_ES___1617639017034.txt\n",
      "gracias por ver el video. ¿con quién tengo el gusto de hablar? con patricia. patricia, ¿dispones del número de pedido? yo no dispongo de nada, yo he hecho el pago, yo he hecho el pago y a mí no me han mandado nada, yo tengo aquí mi pago hecho con el banco. vale, me podría, un momento patricia, para \n",
      "----\n",
      "Source: 2020_12_21_LUFR_529988843072___EMMA_ES__.txt\n",
      "también nosotros nos encontramos en mercado libre. también en amazon, donde se vende el producto. la única diferencia de este... puede ser un poco el precio, porque es otra plataforma, y también los métodos de pago. dígame, por favor, ¿qué tamaño desea? mire, yo quiero un individual, pero el essenti\n"
     ]
    }
   ],
   "source": [
    "# Quick Retrieval Test\n",
    "\n",
    "query = \"How does payment system works using Paypal at agency?\"\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "for r in results:\n",
    "    print(\"----\")\n",
    "    print(\"Source:\", r.metadata[\"source\"])\n",
    "    print(r.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a18aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
