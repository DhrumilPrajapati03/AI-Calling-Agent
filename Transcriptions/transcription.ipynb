{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4672af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "Detected device -> cpu\n"
     ]
    }
   ],
   "source": [
    "# Run this cell. If torch is missing it'll print recommended install commands.\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Detected device ->\", device)\n",
    "except Exception as e:\n",
    "    print(\"PyTorch not found or failed to import.\")\n",
    "    print()\n",
    "    print(\"If you have a CUDA GPU (example for CUDA 11.8):\")\n",
    "    print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print()\n",
    "    print(\"If you want CPU-only (slower):\")\n",
    "    print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9f525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace token (hidden).\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have a .env file in your project root or set HUGGINGFACE_TOKEN env var.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # loads .env into environment if present\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\") or os.getenv(\"HF_TOKEN\") or os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "if not HUGGINGFACE_TOKEN:\n",
    "    raise RuntimeError(\n",
    "        \"HUGGINGFACE_TOKEN not found in env or .env. \"\n",
    "        \"Create a token at https://huggingface.co/settings/tokens and place it in your .env as HUGGINGFACE_TOKEN=\\\"hf_xxx\\\"\"\n",
    "    )\n",
    "\n",
    "print(\"Found HuggingFace token (hidden).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1756d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from faster_whisper import WhisperModel\n",
    "from pyannote.audio import Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Paths & model settings\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "EMMA_DIR = Path(\"D:/Calling agent/S3 bucket Extraction/EMMA_ES\")\n",
    "OUTPUT_DIR = Path(\"D:/Calling agent/S3 bucket Extraction/whisper_diarization_out\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model choices (tweak if needed)\n",
    "MODEL_SIZE = \"small\"          # \"medium\" is a good T4 balance; use \"small\" if you're low on VRAM or CPU-only\n",
    "COMPUTE_TYPE = \"int8\"          # memory-friendly; fallback handled when loading\n",
    "DEVICE = \"cuda\" if ( __import__(\"torch\").cuda.is_available() ) else \"cpu\"\n",
    "\n",
    "# File extensions to process\n",
    "AUDIO_EXTS = {\".wav\", \".mp3\", \".m4a\", \".flac\", \".ogg\", \".webm\", \".aac\"}\n",
    "\n",
    "# Helpers: time formatting for SRT\n",
    "from datetime import timedelta\n",
    "def sec_to_srt_time(t: float) -> str:\n",
    "    if t is None:\n",
    "        t = 0.0\n",
    "    total_seconds = int(t)\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    millis = int((t - total_seconds) * 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{millis:03d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec646447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Loading Faster-Whisper model: small compute_type: int8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Calling agent\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--Systran--faster-whisper-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR model loaded.\n",
      "Loading pyannote speaker-diarization pipeline (this may download model weights)...\n",
      "\n",
      "Could not download 'pyannote/speaker-diarization-3.1' pipeline.\n",
      "It might be because the pipeline is private or gated so make\n",
      "sure to authenticate. Visit https://hf.co/settings/tokens to\n",
      "create your access token and retry with:\n",
      "\n",
      "   >>> Pipeline.from_pretrained('pyannote/speaker-diarization-3.1',\n",
      "   ...                          use_auth_token=YOUR_AUTH_TOKEN)\n",
      "\n",
      "If this still does not work, it might be because the pipeline is gated:\n",
      "visit https://hf.co/pyannote/speaker-diarization-3.1 to accept the user conditions.\n",
      "Pyannote running on CPU (this is slower).\n"
     ]
    }
   ],
   "source": [
    "# Load Faster-Whisper model with a safe fallback if compute_type fails\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Loading Faster-Whisper model:\", MODEL_SIZE, \"compute_type:\", COMPUTE_TYPE)\n",
    "\n",
    "try:\n",
    "    asr_model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE_TYPE)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load with compute_type =\", COMPUTE_TYPE, \"-> trying fallback compute_type='int8_float16' then 'float32'\")\n",
    "    try:\n",
    "        asr_model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=\"int8_float16\")\n",
    "    except Exception:\n",
    "        asr_model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=\"float32\")\n",
    "\n",
    "print(\"ASR model loaded.\")\n",
    "\n",
    "# Load pyannote diarization pipeline\n",
    "print(\"Loading pyannote speaker-diarization pipeline (this may download model weights)...\")\n",
    "diar_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Move pipeline to GPU if available\n",
    "if DEVICE == \"cuda\":\n",
    "    try:\n",
    "        diar_pipeline = diar_pipeline.to(\"cuda\")\n",
    "        print(\"Pyannote pipeline moved to CUDA.\")\n",
    "    except Exception:\n",
    "        print(\"Could not move pyannote to CUDA. Proceeding on CPU.\")\n",
    "else:\n",
    "    print(\"Pyannote running on CPU (this is slower).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fce9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_speakers_to_asr(asr_segments, diarization_annotation):\n",
    "    \"\"\"\n",
    "    asr_segments: list of dicts with keys 'start','end','text'\n",
    "    diarization_annotation: pyannote.annotation.Annotation (returned by pipeline)\n",
    "    returns merged list of dicts with 'start','end','speaker','text'\n",
    "    \"\"\"\n",
    "    # Build diarization segments list [(start,end,speaker_label), ...]\n",
    "    diar_segments = []\n",
    "    for turn, _, label in diarization_annotation.itertracks(yield_label=True):\n",
    "        diar_segments.append({\"start\": turn.start, \"end\": turn.end, \"label\": label})\n",
    "    # Map raw labels to friendly names (Speaker 1, Speaker 2, ...)\n",
    "    unique_labels = sorted({d[\"label\"] for d in diar_segments})\n",
    "    label_map = {lab: f\"Speaker {i+1}\" for i, lab in enumerate(unique_labels)}\n",
    "\n",
    "    merged = []\n",
    "    for s in asr_segments:\n",
    "        s_start, s_end = s[\"start\"], s[\"end\"]\n",
    "        best_label = None\n",
    "        best_overlap = 0.0\n",
    "        # score by overlap duration\n",
    "        for d in diar_segments:\n",
    "            overlap = max(0.0, min(s_end, d[\"end\"]) - max(s_start, d[\"start\"]))\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_label = d[\"label\"]\n",
    "\n",
    "        # fallback: if no overlap, pick diar segment containing midpoint\n",
    "        if best_overlap == 0.0:\n",
    "            midpoint = (s_start + s_end) / 2.0\n",
    "            for d in diar_segments:\n",
    "                if d[\"start\"] <= midpoint < d[\"end\"]:\n",
    "                    best_label = d[\"label\"]\n",
    "                    break\n",
    "\n",
    "        speaker = label_map.get(best_label, \"Unknown\")\n",
    "        merged.append({\n",
    "            \"start\": s_start,\n",
    "            \"end\": s_end,\n",
    "            \"speaker\": speaker,\n",
    "            \"text\": s[\"text\"]\n",
    "        })\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4d42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(merged_segments, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_segments, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def save_srt(merged_segments, out_path):\n",
    "    lines = []\n",
    "    for i, seg in enumerate(merged_segments, start=1):\n",
    "        start_s = sec_to_srt_time(seg[\"start\"])\n",
    "        end_s = sec_to_srt_time(seg[\"end\"])\n",
    "        text = f\"{seg['speaker']}: {seg['text']}\"\n",
    "        lines.append(str(i))\n",
    "        lines.append(f\"{start_s} --> {end_s}\")\n",
    "        lines.append(text)\n",
    "        lines.append(\"\")  # blank line\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153fcde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 2020_12_21_LUFR_529988843072___EMMA_ES__.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/16 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 1) ASR (Faster-Whisper)\u001b[39;00m\n\u001b[32m     13\u001b[39m asr_segments = []\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m segments, info = \u001b[43masr_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvad_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# adjust beam_size as needed\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seg \u001b[38;5;129;01min\u001b[39;00m segments:\n\u001b[32m     16\u001b[39m     asr_segments.append({\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m: seg.start,\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m: seg.end,\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: seg.text.strip()\n\u001b[32m     20\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Calling agent\\myenv\\Lib\\site-packages\\faster_whisper\\transcribe.py:916\u001b[39m, in \u001b[36mWhisperModel.transcribe\u001b[39m\u001b[34m(self, audio, language, task, log_progress, beam_size, best_of, patience, length_penalty, repetition_penalty, no_repeat_ngram_size, temperature, compression_ratio_threshold, log_prob_threshold, no_speech_threshold, condition_on_previous_text, prompt_reset_on_temperature, initial_prompt, prefix, suppress_blank, suppress_tokens, without_timestamps, max_initial_timestamp, word_timestamps, prepend_punctuations, append_punctuations, multilingual, vad_filter, vad_parameters, max_new_tokens, chunk_length, clip_timestamps, hallucination_silence_threshold, hotwords, language_detection_threshold, language_detection_segments)\u001b[39m\n\u001b[32m    906\u001b[39m         content_frames = features.shape[-\u001b[32m1\u001b[39m] - \u001b[32m1\u001b[39m\n\u001b[32m    907\u001b[39m         seek = (\n\u001b[32m    908\u001b[39m             \u001b[38;5;28mint\u001b[39m(start_timestamp * \u001b[38;5;28mself\u001b[39m.frames_per_second)\n\u001b[32m    909\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m start_timestamp * \u001b[38;5;28mself\u001b[39m.frames_per_second < content_frames\n\u001b[32m    910\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    911\u001b[39m         )\n\u001b[32m    912\u001b[39m         (\n\u001b[32m    913\u001b[39m             language,\n\u001b[32m    914\u001b[39m             language_probability,\n\u001b[32m    915\u001b[39m             all_language_probs,\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m         ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguage_detection_segments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_detection_segments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguage_detection_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_detection_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    922\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.info(\n\u001b[32m    923\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDetected language \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m with probability \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    924\u001b[39m             language,\n\u001b[32m    925\u001b[39m             language_probability,\n\u001b[32m    926\u001b[39m         )\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Calling agent\\myenv\\Lib\\site-packages\\faster_whisper\\transcribe.py:1793\u001b[39m, in \u001b[36mWhisperModel.detect_language\u001b[39m\u001b[34m(self, audio, features, vad_filter, vad_parameters, language_detection_segments, language_detection_threshold)\u001b[39m\n\u001b[32m   1791\u001b[39m detected_language_info = {}\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, features.shape[-\u001b[32m1\u001b[39m], \u001b[38;5;28mself\u001b[39m.feature_extractor.nb_max_frames):\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     encoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_or_trim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnb_max_frames\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# results is a list of tuple[str, float] with language names and probabilities.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.model.detect_language(encoder_output)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Calling agent\\myenv\\Lib\\site-packages\\faster_whisper\\transcribe.py:1374\u001b[39m, in \u001b[36mWhisperModel.encode\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m   1371\u001b[39m     features = np.expand_dims(features, \u001b[32m0\u001b[39m)\n\u001b[32m   1372\u001b[39m features = get_ctranslate2_storage(features)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_cpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not EMMA_DIR.exists():\n",
    "    raise RuntimeError(f\"EMMA_ES folder not found at {EMMA_DIR}. Update EMMA_DIR path and re-run.\")\n",
    "\n",
    "audio_files = sorted([p for p in EMMA_DIR.iterdir() if p.suffix.lower() in AUDIO_EXTS])\n",
    "print(\"Files to process:\", len(audio_files))\n",
    "\n",
    "for audio_path in tqdm(audio_files, desc=\"Files\"):\n",
    "    print(\"\\nProcessing:\", audio_path.name)\n",
    "    # 1) ASR (Faster-Whisper)\n",
    "    asr_segments = []\n",
    "    segments, info = asr_model.transcribe(str(audio_path), beam_size=5, vad_filter=True)  # adjust beam_size as needed\n",
    "    for seg in segments:\n",
    "        asr_segments.append({\n",
    "            \"start\": seg.start,\n",
    "            \"end\": seg.end,\n",
    "            \"text\": seg.text.strip()\n",
    "        })\n",
    "\n",
    "    # 2) Diarization (pyannote)\n",
    "    diar = diar_pipeline(str(audio_path))\n",
    "\n",
    "    # 3) Merge\n",
    "    merged = assign_speakers_to_asr(asr_segments, diar)\n",
    "\n",
    "    # 4) Save\n",
    "    base = OUTPUT_DIR / audio_path.stem\n",
    "    save_json(merged, str(base.with_suffix(\".json\")))\n",
    "    save_srt(merged, str(base.with_suffix(\".srt\")))\n",
    "    print(\"Saved:\", base.with_suffix(\".json\").name, \"and\", base.with_suffix(\".srt\").name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5cd023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a quick summary of all outputs\n",
    "outs = sorted(OUTPUT_DIR.glob(\"*\"))\n",
    "print(\"Output files in\", OUTPUT_DIR)\n",
    "for f in outs:\n",
    "    print(\"-\", f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0238258",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57bdab22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92cea30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in d:\\calling agent\\myenv\\lib\\site-packages (20250625)\n",
      "Requirement already satisfied: more-itertools in d:\\calling agent\\myenv\\lib\\site-packages (from openai-whisper) (10.7.0)\n",
      "Requirement already satisfied: numba in d:\\calling agent\\myenv\\lib\\site-packages (from openai-whisper) (0.61.2)\n",
      "Requirement already satisfied: numpy in d:\\calling agent\\myenv\\lib\\site-packages (from openai-whisper) (2.2.6)\n",
      "Requirement already satisfied: tiktoken in d:\\calling agent\\myenv\\lib\\site-packages (from openai-whisper) (0.11.0)\n",
      "Requirement already satisfied: torch in d:\\calling agent\\myenv\\lib\\site-packages (from openai-whisper) (2.8.0)\n",
      "Requirement already satisfied: tqdm in d:\\calling agent\\myenv\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in d:\\calling agent\\myenv\\lib\\site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\calling agent\\myenv\\lib\\site-packages (from tiktoken->openai-whisper) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\calling agent\\myenv\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
      "Requirement already satisfied: filelock in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in d:\\calling agent\\myenv\\lib\\site-packages (from torch->openai-whisper) (80.9.0)\n",
      "Requirement already satisfied: colorama in d:\\calling agent\\myenv\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\calling agent\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\calling agent\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\calling agent\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\calling agent\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\calling agent\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\calling agent\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33f30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
